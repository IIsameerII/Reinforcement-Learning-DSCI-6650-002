{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Method with Exploring Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (0, 1): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (0, 3): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (0, 4): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (1, 0): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (1, 1): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (1, 3): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (1, 4): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (2, 0): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (2, 1): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (2, 3): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (2, 4): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (3, 0): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (3, 1): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (3, 3): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (3, 4): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (4, 0): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (4, 1): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (4, 3): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}, (4, 4): {'U': 0.25, 'D': 0.25, 'L': 0.25, 'R': 0.25}}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def generate_episode_from_exploring_starts(policy, grid, terminal_states):\n",
    "    # Randomly choose an initial state and action from non-terminal states\n",
    "    non_terminal_states = [state for state in policy.keys() if state not in terminal_states]\n",
    "    start_state = random.choice(non_terminal_states)\n",
    "    start_action = np.random.choice(list(policy[start_state].keys()), p=list(policy[start_state].values()))\n",
    "    episode = [(start_state, start_action, 0)]  # reward for the first action is 0 as it's part of the setup\n",
    "    current_state = start_state\n",
    "    while current_state not in terminal_states:\n",
    "        next_state, reward = step(current_state, start_action, grid, terminal_states)\n",
    "        episode.append((next_state, start_action, reward))\n",
    "        if next_state in terminal_states:\n",
    "            break\n",
    "        current_state = next_state\n",
    "        start_action = np.random.choice(list(policy[current_state].keys()), p=list(policy[current_state].values()))\n",
    "    return episode\n",
    "\n",
    "def step(state, action, grid, terminal_states):\n",
    "    n = len(grid)\n",
    "    x, y = state\n",
    "    if action == 'U' and x > 0:\n",
    "        x -= 1\n",
    "    elif action == 'D' and x < n - 1:\n",
    "        x += 1\n",
    "    elif action == 'L' and y > 0:\n",
    "        y -= 1\n",
    "    elif action == 'R' and y < n - 1:\n",
    "        y += 1\n",
    "    next_state = (x, y)\n",
    "    reward = -0.2 if next_state not in terminal_states and next_state != state else 0\n",
    "    return next_state, reward\n",
    "\n",
    "def mc_control_exploring_starts(grid, terminal_states, episodes, gamma=0.95):\n",
    "    # Initialize policy as a distribution over actions for non-terminal states\n",
    "    policy = {}\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    for i in range(len(grid)):\n",
    "        for j in range(len(grid[0])):\n",
    "            if (i, j) not in terminal_states:\n",
    "                actions = ['U', 'D', 'L', 'R']\n",
    "                policy[(i, j)] = {action: 1.0 / len(actions) for action in actions}  # Uniform distribution initially\n",
    "                Q[(i, j)] = {action: 0 for action in actions}\n",
    "                returns[(i, j)] = {action: [] for action in actions}\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        episode = generate_episode_from_exploring_starts(policy, grid, terminal_states)\n",
    "        G = 0\n",
    "        for state, action, reward in reversed(episode):\n",
    "            if state not in terminal_states:\n",
    "                G = gamma * G + reward\n",
    "                if not any(x[0] == state and x[1] == action for x in episode[:-1]):\n",
    "                    returns[state][action].append(G)\n",
    "                    Q[state][action] = np.mean(returns[state][action])\n",
    "                    best_action = max(Q[state], key=Q[state].get)\n",
    "                    # Policy improvement\n",
    "                    policy[state] = {a: 0.1 / len(actions) for a in actions}\n",
    "                    policy[state][best_action] += 0.9\n",
    "\n",
    "    return policy\n",
    "\n",
    "# Define the grid and terminal states\n",
    "grid = [[0] * 5 for _ in range(5)]  # Simplified grid representation\n",
    "terminal_states = [(0, 2), (1, 2), (2, 2), (3, 2), (4, 2)]  # Black squares as terminal states\n",
    "\n",
    "# Run Monte Carlo Control with Exploring Starts\n",
    "optimal_policy = mc_control_exploring_starts(grid, terminal_states, 10000)\n",
    "print(optimal_policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo with ε-soft Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): array([0.25, 0.25, 0.25, 0.25]), (0, 1): array([0.25, 0.25, 0.25, 0.25]), (0, 3): array([0.25, 0.25, 0.25, 0.25]), (0, 4): array([0.25, 0.25, 0.25, 0.25]), (1, 0): array([0.25, 0.25, 0.25, 0.25]), (1, 1): array([0.25, 0.25, 0.25, 0.25]), (1, 3): array([0.25, 0.25, 0.25, 0.25]), (1, 4): array([0.25, 0.25, 0.25, 0.25]), (2, 0): array([0.25, 0.25, 0.25, 0.25]), (2, 1): array([0.25, 0.25, 0.25, 0.25]), (2, 3): array([0.25, 0.25, 0.25, 0.25]), (2, 4): array([0.25, 0.25, 0.25, 0.25]), (3, 0): array([0.25, 0.25, 0.25, 0.25]), (3, 1): array([0.25, 0.25, 0.25, 0.25]), (3, 3): array([0.25, 0.25, 0.25, 0.25]), (3, 4): array([0.25, 0.25, 0.25, 0.25]), (4, 0): array([0.25, 0.25, 0.25, 0.25]), (4, 1): array([0.25, 0.25, 0.25, 0.25]), (4, 3): array([0.25, 0.25, 0.25, 0.25]), (4, 4): array([0.25, 0.25, 0.25, 0.25])}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def initialize_grid():\n",
    "    # Create a 5x5 grid with specific states labeled\n",
    "    grid = np.zeros((5, 5))\n",
    "    terminal_states = [(0, 2), (1, 2), (2, 2), (3, 2), (4, 2)]  # Black squares as terminal states\n",
    "    special_rewards = {(0, 1): 5, (0, 3): 2.5, (1, 1): 0.5, (1, 3): -1}  # Blue, Green, Red, Yellow\n",
    "    return grid, terminal_states, special_rewards\n",
    "\n",
    "def step(state, action, terminal_states, special_rewards):\n",
    "    x, y = state\n",
    "    if action == 'U':\n",
    "        x = max(0, x-1)\n",
    "    elif action == 'D':\n",
    "        x = min(4, x+1)\n",
    "    elif action == 'L':\n",
    "        y = max(0, y-1)\n",
    "    elif action == 'R':\n",
    "        y = min(4, y+1)\n",
    "    \n",
    "    next_state = (x, y)\n",
    "    if next_state in terminal_states:\n",
    "        reward = 0  # No reward for entering a terminal state\n",
    "    elif next_state in special_rewards:\n",
    "        reward = special_rewards[next_state]  # Special rewards for colored squares\n",
    "    else:\n",
    "        reward = -0.2  # Standard movement penalty\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "def generate_episode(policy, grid, terminal_states, special_rewards):\n",
    "    start_state = (2, 2)  # Start from the center of the grid or another non-terminal state\n",
    "    episode = []\n",
    "    state = start_state\n",
    "    while state not in terminal_states:\n",
    "        action = np.random.choice(['U', 'D', 'L', 'R'], p=policy[state])\n",
    "        next_state, reward = step(state, action, terminal_states, special_rewards)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "def mc_control_epsilon_soft(grid, terminal_states, special_rewards, episodes, epsilon=0.1, gamma=0.95):\n",
    "    policy = {(i, j): np.array([0.25, 0.25, 0.25, 0.25]) for i in range(5) for j in range(5) if (i, j) not in terminal_states}\n",
    "    Q = {(i, j): np.zeros(4) for i in range(5) for j in range(5) if (i, j) not in terminal_states}\n",
    "    returns = {(i, j): [[] for _ in range(4)] for i in range(5) for j in range(5) if (i, j) not in terminal_states}\n",
    "\n",
    "    for episode_num in range(episodes):\n",
    "        episode = generate_episode(policy, grid, terminal_states, special_rewards)\n",
    "        G = 0\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = gamma * G + reward\n",
    "            action_index = ['U', 'D', 'L', 'R'].index(action)\n",
    "            if (state, action_index) not in [(x[0], ['U', 'D', 'L', 'R'].index(x[1])) for x in episode[:episode.index((state, action, reward))]]:\n",
    "                returns[state][action_index].append(G)\n",
    "                Q[state][action_index] = np.mean(returns[state][action_index])\n",
    "                best_action_index = np.argmax(Q[state])\n",
    "                policy[state] = np.ones(4) * epsilon / 4\n",
    "                policy[state][best_action_index] += (1 - epsilon)\n",
    "\n",
    "    return policy\n",
    "\n",
    "# Initialize the grid and related structures\n",
    "grid, terminal_states, special_rewards = initialize_grid()\n",
    "# Run the Monte Carlo control with ε-soft policy\n",
    "optimal_policy = mc_control_epsilon_soft(grid, terminal_states, special_rewards, 10000)\n",
    "print(optimal_policy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "not_base_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
